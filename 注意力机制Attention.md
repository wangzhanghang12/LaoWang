## 注意力机制Attention

最基本的注意力机制是加性注意力。它计算输入序列各个位置的注意力权重的加权和,并使用这个加权表示作为上下文向量馈入解码器。具体来说,它的计算公式如下:

```
Attention(Q, K, V) = softmax(QK^T)V
```

其中:- Q 是解码器的查询向量
\- K 是输入序列的密钥向量
\- V 是输入序列的值向量
\- softmax函数将注意力权重归一化为0到1之间加性注意力允许模型明确地"对齐"输入和输出序列,这对于像机器翻译这样的任务尤其重要。更复杂的注意力机制包括:- 点积注意力:它考虑Q和K的点积,而不仅仅是它们的加权和。
\- 双向注意力:不仅解码器查询输入序列,输入序列也查询解码器生成的输出序列。这可以产生更丰富的上下文向量。
\- 多头注意力:使用多个注意力头并结合结果,可以 jointly 关注输入序列的不同方面。

### 普适的注意力机制

比如encoder decoder， key就是我们的encoder的RNN的隐藏输出, query就是我们decoder RNN的隐藏层输出, Value可以当作是输入的句子而a就是要帮我们把和相对来说无关的值滤出去。

### Hierachical Attention

分层注意力是一种更加复杂的注意力机制,用于序列到序列模型。它允许模型在不同的级别对齐输入和输出序列。最简单的分层注意力是双向注意力,它包含:- 词级注意力:标准的加性注意力,对齐输入和输出序列的单词。
\- 句子级注意力:一个额外的注意力层,对齐输入和输出序列的句子。它使用编码器输出的句子表示,以及解码器生成的当前句子的表示,计算句子级注意力。然后,这两种注意力的注意力权重都用于计算上下文向量,该向量被馈入解码器。这样,解码器不仅考虑当前生成的单词,还考虑当前生成的句子以及输入序列的相关句子。这可以产生更丰富和准确的上下文向量,从而提高翻译质量。除了词级和句子级注意力,我们还可以有:- 篇章级注意力:对齐输入和输出篇章
\- 文档级注意力:对齐输入和输出文档
等等。总的来说,分层注意力机制允许模型在多个级别建立输入和输出序列之间的对齐。这些不同粒度的注意力可以被联合使用,为序列到序列任务提供有用的上下文信息,提高模型的性能。分层注意力已被证明在许多任务中提高序列到序列模型的效果,例如机器翻译、文本摘要、阅读理解等等。它通过考虑输入和输出序列在不同级别上的关系,为解码器提供更丰富的上下文,这有助于生成更准确和连贯的输出序列。

### Transformer

转换器(Transformer)是一种基于注意力机制的神经网络,用于序列到序列学习。与基于RNN的模型不同,Transformer不依赖于序列序列表示,而是依赖注意力机制建模序列之间的全局依赖关系。Transformer由三个基本模块组成:

1.多头自注意力(Multi-Head Self-Attention):该模块利用注意力机制捕获输入序列中的全局依赖关系。它包含多个并行的注意力头,这些注意力头联合生成最终的注意力输出。

2.前馈神经网络:该模块实现输入的非线性映射。它通常包含两个线性变换,中间加ReLU激活函数。

3.按位置编码(Positional Encoding):由于Transformer不像RNN/LSTM那样拥有内在的序列序列表示,我们需要为输入序列加入位置编码,以便区分不同位置的词义。位置编码很简单,只是把位置索引映射为稍微复杂一点的向量。Transformer的基本架构包含:- 编码器(Encoder):N个相同的层,每层包含多头自注意力和前馈神经网络。
\- 解码器(Decoder):N个相同的层,每层包含自注意力、多头注意力(encoder-decoder attention)和前馈神经网络。
\- 最终的线性映射和softmax function以产生输出词汇表的概率分布。Transformer的强大之处在于它完全基于注意力机制,在编码器和解码器之间建立全局的依赖关系。相比于RNN/LSTM等模型,它的并行性更强,训练速度更快。Transformer已经在许多NLP任务中取得了最佳效果,包括机器翻译、文本摘要、问答等。

 

#### position Encoding

按位置编码(Positional Encoding)是Transformer模型中一个重要的概念。由于Transformer不像RNN/LSTM那样有内在的序列序列表示,因此需要为输入序列中的每个词添加位置信息,以区分不同位置的词的含义。位置编码的目的是在Transformer的注意力机制中为每个词编码其在序列中的位置信息。这是通过使用正弦和余弦函数对词索引进行编码实现的:

```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

其中,pos是词的位置,i是维度索引,d_model是位置编码的embedding大小。比如,如果d_model=512,则位置编码向量PE的第1、2、3、4维分别是:sin(pos / 10000^(2*0/512))
cos(pos / 10000^(2*0/512))
sin(pos / 10000^(2*1/512)) 
cos(pos / 10000^(2*1/512))
...可以看出,位置编码向量的第i维和第i+1维表示一个正弦曲线和余弦曲线。这些曲线的频率是随着维度指数提高而提高的。这样的位置编码使得模型可以学会词的顺序关系。例如,相邻词的位置编码向量差异较小,而远距离词的位置编码差异较大。Transformer通过添加这种用于编码顺序信息的信号,可以有效地进行词义消歧和句法解析。总的来说,位置编码给输入序列的每个词提供了一个唯一的向量,用于在Transformer的注意力机制中区分不同位置的词。这种简单而有效的方法使Transformer可以建模词序,并在Encoder和Decoder中实现全局的词元依赖。

### BERT

BERT(Bidirectional Encoder Representations from Transformers)是一种由谷歌研究开发的预训练语言表示方法。它的目标是在大量未标注语料上进行预训练,并在各种下游NLP任务上进行微调,从而达到出色的性能。BERT基于Transformer编码器,它通过遮掩某些词并预测这些词来预训练。这种遮掩语言模型可以捕获词与其上下文的双向关系,产生一个通用的语言表达。

BERT有两个版本:

\-BERTBASE:12层Transformer,768隐藏层节点,12注意力头。
\- BERTLARGE:24层Transformer,1024隐藏层节点,16注意力头。

BERT的预训练采用两个无监督任务:

1. 遮掩语言模型:随机遮掩输入序列的15%词元,并预测这些被遮掩的词元。这要求模型理解每个词元及其上下文。

2. 下一句预测:给定两句话A和B,模型需要判断B是否是A的下一句。这要求模型理解句子之间的关系。BERT通过大量数据 joints 训练这两个无监督任务,并实现对词、句子和句子之间关系的深层理解。

   BERT有3个版本:

   - BERTBASE: Loads BERTBASE model, with 12-layer, 768-hidden, 12-heads, 110M parameters.
     \- BERTLARGE: Loads BERTLARGE model, with 24-layer, 1024-hidden, 16-heads, 340M parameters.
     \- BERTForPreTraining: The model used for pre-training BERT. It includes the masked language model and next sentence prediction objectives.可以将BERT与常用的NLP任务一起微调,这包括:

     分类:将BERT的输出向量序列fed到分类层。
     \- 问答:将问题和文章序列fed到BERT,使用输出来预测起始和结束位置。
     \- 命名实体识别:将输入序列fed到BERT,并使用输出预测每个词的NER标签。
     \- 等等。BERT已经在这些任务上取得了最佳效果,标志着NLP和预训练语言表达的新时代。

### 论文推荐

![image-20230623154134466](C:\Users\wangzhanghang\AppData\Roaming\Typora\typora-user-images\image-20230623154134466.png)